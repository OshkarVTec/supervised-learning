{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a208b138",
   "metadata": {},
   "source": [
    "# Actividad 2.2: Aprendizaje supervisado\n",
    "Oskar Villa\n",
    "\n",
    "Cruz Pérez\n",
    "\n",
    "Rogelio Hernández"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2542d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea09e0fa",
   "metadata": {},
   "source": [
    "## Investigación documental: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cc4619",
   "metadata": {},
   "source": [
    "### Descripción General\n",
    "\n",
    "El clasificador **Random Forest** es un algoritmo de aprendizaje supervisado que pertenece a la categoría de métodos de *ensamble*. Su funcionamiento se basa en la construcción de múltiples árboles de decisión durante el entrenamiento, cuya salida se combina para producir una predicción final más precisa y robusta. Para tareas de clasificación, el resultado se determina por votación mayoritaria entre los árboles individuales.\n",
    "\n",
    "### Funcionamiento\n",
    "\n",
    "Random Forest emplea la técnica de *bootstrap aggregating* (también conocida como *bagging*), que consiste en:\n",
    "\n",
    "1. Generar subconjuntos aleatorios del conjunto de datos original, con reemplazo.\n",
    "2. Entrenar un árbol de decisión independiente en cada subconjunto.\n",
    "3. En cada división del árbol, se selecciona aleatoriamente un subconjunto de características para elegir la mejor partición.\n",
    "4. La predicción final se realiza mediante votación entre los árboles (en clasificación) o promedio (en regresión).\n",
    "\n",
    "Este enfoque permite reducir la varianza del modelo y mejora su capacidad de generalización.\n",
    "\n",
    "### Tipos de Datos que Maneja\n",
    "\n",
    "El clasificador Random Forest puede manejar:\n",
    "\n",
    "- Datos estructurados en formato tabular.\n",
    "- Variables tanto numéricas como categóricas.\n",
    "- Conjuntos de datos con valores faltantes o ruidosos.\n",
    "- Variables no escaladas (no requiere normalización o estandarización).\n",
    "\n",
    "### Principales Parámetros\n",
    "\n",
    "| Parámetro            | Descripción                                                                 |\n",
    "|----------------------|-----------------------------------------------------------------------------|\n",
    "| `n_estimators`       | Número de árboles que compondrán el bosque.                                |\n",
    "| `max_depth`          | Profundidad máxima permitida de los árboles.                               |\n",
    "| `min_samples_split`  | Número mínimo de muestras requeridas para dividir un nodo.                 |\n",
    "| `max_features`       | Número máximo de características consideradas para dividir en cada nodo.   |\n",
    "| `bootstrap`          | Indica si se utiliza muestreo con reemplazo al generar los subconjuntos.  |\n",
    "| `criterion`          | Función empleada para medir la calidad de una división (`gini`, `entropy`).|\n",
    "\n",
    "\n",
    "### Referencias\n",
    "\n",
    "- Breiman, L. (2001). *Random Forests*. *Machine Learning*, 45(1), 5–32.  \n",
    "  [https://link.springer.com/article/10.1023/A:1010933404324](https://link.springer.com/article/10.1023/A:1010933404324)\n",
    "- Scikit-learn documentation. *Random Forest Classifier*.  \n",
    "  [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "- Géron, A. (2019). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a2898c",
   "metadata": {},
   "source": [
    "## Datos y objetivo de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25556618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White Wine Quality Counts:\n",
      "quality\n",
      "6    2198\n",
      "5    1457\n",
      "7     880\n",
      "8     175\n",
      "4     163\n",
      "3      20\n",
      "9       5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Red Wine Quality Counts:\n",
      "quality\n",
      "5    681\n",
      "6    638\n",
      "7    199\n",
      "4     53\n",
      "8     18\n",
      "3     10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the white wine and red wine datasets\n",
    "white_wine_data = pd.read_csv('data/winequality-white.csv')\n",
    "red_wine_data = pd.read_csv('data/winequality-red.csv')\n",
    "\n",
    "# Count the number of appearances of each class in the 'quality' column for each dataset\n",
    "white_wine_quality_counts = white_wine_data['quality'].value_counts()\n",
    "red_wine_quality_counts = red_wine_data['quality'].value_counts()\n",
    "\n",
    "print(\"White Wine Quality Counts:\")\n",
    "print(white_wine_quality_counts)\n",
    "\n",
    "print(\"\\nRed Wine Quality Counts:\")\n",
    "print(red_wine_quality_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd91bd",
   "metadata": {},
   "source": [
    "### Objetivo de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4c0e1",
   "metadata": {},
   "source": [
    "Para mantener un balance de clases, se seleccionaron los siguientes datos:\n",
    "* Se utilizará vino blanco (white)\n",
    "* Se usarán las clases 4 y 8. La clase objetivo es 4.\n",
    "* Las métricas de evaluación serán accuracy y F1 score, ya que F1 es una métrica que combina Recall y Precision. Para tener una mejor representación, se utilizará el **promedio ponderado** de F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d9425",
   "metadata": {},
   "source": [
    "### Entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d2bb6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set label distribution:\n",
      "quality\n",
      "8    122\n",
      "4    114\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set label distribution:\n",
      "quality\n",
      "8    53\n",
      "4    49\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter the white wine dataset to include only quality 4 and 8\n",
    "filtered_white_wine = white_wine_data[white_wine_data['quality'].isin([4, 8])]\n",
    "\n",
    "# Separate features and target\n",
    "X = filtered_white_wine.drop(columns=['quality'])\n",
    "y = filtered_white_wine['quality']\n",
    "\n",
    "# Perform train-test split with stratification to maintain label balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Verify the balance of labels in both splits\n",
    "print(\"Training set label distribution:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"\\nTest set label distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1490c399",
   "metadata": {},
   "source": [
    "### Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cadd8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.918220</td>\n",
       "      <td>0.326059</td>\n",
       "      <td>0.317288</td>\n",
       "      <td>5.039831</td>\n",
       "      <td>0.045237</td>\n",
       "      <td>30.065678</td>\n",
       "      <td>126.542373</td>\n",
       "      <td>0.993330</td>\n",
       "      <td>3.210763</td>\n",
       "      <td>0.488051</td>\n",
       "      <td>10.877966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.004059</td>\n",
       "      <td>0.148333</td>\n",
       "      <td>0.128789</td>\n",
       "      <td>4.278606</td>\n",
       "      <td>0.023650</td>\n",
       "      <td>20.253577</td>\n",
       "      <td>45.314085</td>\n",
       "      <td>0.002831</td>\n",
       "      <td>0.159046</td>\n",
       "      <td>0.129205</td>\n",
       "      <td>1.343963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.400000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.987130</td>\n",
       "      <td>2.830000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>8.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>0.227500</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>1.587500</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.991038</td>\n",
       "      <td>3.090000</td>\n",
       "      <td>0.387500</td>\n",
       "      <td>9.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.900000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>0.993020</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>10.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.400000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>0.050250</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>0.995138</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.200000</td>\n",
       "      <td>1.005000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>15.400000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>138.500000</td>\n",
       "      <td>272.000000</td>\n",
       "      <td>1.000600</td>\n",
       "      <td>3.720000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count     236.000000        236.000000   236.000000      236.000000   \n",
       "mean        6.918220          0.326059     0.317288        5.039831   \n",
       "std         1.004059          0.148333     0.128789        4.278606   \n",
       "min         4.400000          0.120000     0.000000        0.700000   \n",
       "25%         6.400000          0.227500     0.260000        1.587500   \n",
       "50%         6.900000          0.290000     0.310000        3.600000   \n",
       "75%         7.400000          0.380000     0.370000        7.250000   \n",
       "max        10.200000          1.005000     0.880000       15.400000   \n",
       "\n",
       "        chlorides  free sulfur dioxide  total sulfur dioxide     density  \\\n",
       "count  236.000000           236.000000            236.000000  236.000000   \n",
       "mean     0.045237            30.065678            126.542373    0.993330   \n",
       "std      0.023650            20.253577             45.314085    0.002831   \n",
       "min      0.013000             3.000000             10.000000    0.987130   \n",
       "25%      0.034000            15.000000             96.000000    0.991038   \n",
       "50%      0.041000            29.000000            122.000000    0.993020   \n",
       "75%      0.050250            39.000000            155.000000    0.995138   \n",
       "max      0.290000           138.500000            272.000000    1.000600   \n",
       "\n",
       "               pH   sulphates     alcohol  \n",
       "count  236.000000  236.000000  236.000000  \n",
       "mean     3.210763    0.488051   10.877966  \n",
       "std      0.159046    0.129205    1.343963  \n",
       "min      2.830000    0.250000    8.400000  \n",
       "25%      3.090000    0.387500    9.800000  \n",
       "50%      3.200000    0.470000   10.900000  \n",
       "75%      3.320000    0.570000   12.000000  \n",
       "max      3.720000    0.950000   14.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ddea38",
   "metadata": {},
   "source": [
    "No hay datos nulos y parecen no contener outliers significativos a simple vista. Solo se realizará normalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87d4db2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Training Data:\n",
      "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "1348       0.586207          0.192090     0.409091        0.428571   0.104693   \n",
      "3186       0.741379          0.203390     0.670455        0.068027   0.119134   \n",
      "1983       0.551724          0.090395     0.340909        0.918367   0.155235   \n",
      "3736       0.362069          0.225989     0.545455        0.496599   0.046931   \n",
      "1029       0.327586          0.542373     0.500000        0.700680   0.133574   \n",
      "\n",
      "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
      "1348             0.258303              0.576336  0.517446  0.483146   \n",
      "3186             0.051661              0.229008  0.489978  0.258427   \n",
      "1983             0.369004              0.772901  0.881218  0.348315   \n",
      "3736             0.110701              0.297710  0.319970  0.438202   \n",
      "1029             0.346863              0.896947  0.747587  0.404494   \n",
      "\n",
      "      sulphates   alcohol  \n",
      "1348   0.171429  0.500000  \n",
      "3186   0.442857  0.428571  \n",
      "1983   0.300000  0.089286  \n",
      "3736   0.771429  0.767857  \n",
      "1029   0.457143  0.160714  \n",
      "\n",
      "Normalized Test Data:\n",
      "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "22         0.413793          0.158192     0.477273        0.068027   0.129964   \n",
      "3484       0.310345          0.282486     0.272727        0.367347   0.068592   \n",
      "626        0.551724          0.497175     0.136364        0.659864   0.299639   \n",
      "2390       0.448276          0.180791     0.295455        0.068027   0.104693   \n",
      "3062       0.413793          0.180791     0.488636        0.469388   0.061372   \n",
      "\n",
      "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
      "22               0.280443              0.427481  0.435783  0.719101   \n",
      "3484             0.118081              0.290076  0.164068  0.235955   \n",
      "626              0.140221              0.637405  0.829250  0.550562   \n",
      "2390             0.228782              0.458015  0.398664  0.674157   \n",
      "3062             0.199262              0.381679  0.334818  0.280899   \n",
      "\n",
      "      sulphates   alcohol  \n",
      "22     0.328571  0.375000  \n",
      "3484   0.014286  0.892857  \n",
      "626    0.285714  0.125000  \n",
      "2390   0.357143  0.410714  \n",
      "3062   0.485714  0.732143  \n"
     ]
    }
   ],
   "source": [
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize the training and test data\n",
    "X_train_normalized = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test_normalized = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# Display the normalized data\n",
    "print(\"Normalized Training Data:\")\n",
    "print(X_train_normalized.head())\n",
    "\n",
    "print(\"\\nNormalized Test Data:\")\n",
    "print(X_test_normalized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068d4b69",
   "metadata": {},
   "source": [
    "## Implementación de algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d2012",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "467696db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train, y_train, X_test, y_test, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Train a KNN classifier and evaluate its performance.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training features\n",
    "    - y_train: Training labels\n",
    "    - X_test: Test features\n",
    "    - y_test: Test labels\n",
    "    - n_neighbors: Number of neighbors for KNN\n",
    "    \n",
    "    Returns:\n",
    "    - accuracy: Accuracy of the model on the test set\n",
    "    - f1: F1 score of the model on the test set\n",
    "    - report: Classification report\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    \n",
    "    # Train the KNN classifier\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = classification_report(y_test, y_pred, output_dict=True)['weighted avg']['f1-score']\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, f1, report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab78ae8",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dcd0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9019607843137255, 0.9019607843137255, '              precision    recall  f1-score   support\\n\\n           4       0.90      0.90      0.90        49\\n           8       0.91      0.91      0.91        53\\n\\n    accuracy                           0.90       102\\n   macro avg       0.90      0.90      0.90       102\\nweighted avg       0.90      0.90      0.90       102\\n')\n"
     ]
    }
   ],
   "source": [
    "def decision_tree(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train a Decision Tree classifier and evaluate its performance.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training features\n",
    "    - y_train: Training labels\n",
    "    - X_test: Test features\n",
    "    - y_test: Test labels\n",
    "    \n",
    "    Returns:\n",
    "    - accuracy: Accuracy of the model on the test set\n",
    "    - f1: F1 score of the model on the test set\n",
    "    - report: Classification report\n",
    "    \"\"\"\n",
    "        \n",
    "    # Initialize the Decision Tree classifier\n",
    "    dt = DecisionTreeClassifier(random_state=42)\n",
    "    \n",
    "    # Train the Decision Tree classifier\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = dt.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = classification_report(y_test, y_pred, output_dict=True)['weighted avg']['f1-score']\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, f1, report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8db311",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a31da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train, X_test, y_test, n_estimators=100, max_depth=None, random_state=42):\n",
    "    \"\"\"\n",
    "    Train a Random Forest classifier and evaluate its performance.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training features\n",
    "    - y_train: Training labels\n",
    "    - X_test: Test features\n",
    "    - y_test: Test labels\n",
    "    - n_estimators: Number of trees in the forest\n",
    "    - max_depth: Maximum depth of the trees\n",
    "    - random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    - accuracy: Accuracy of the model on the test set\n",
    "    - f1: F1 score of the model on the test set\n",
    "    - report: Classification report\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the Random Forest classifier\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n",
    "    \n",
    "    # Train the Random Forest classifier\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = classification_report(y_test, y_pred, output_dict=True)['weighted avg']['f1-score']\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, f1, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a556a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9019607843137255, 0.9010434583190001, '              precision    recall  f1-score   support\\n\\n           4       0.98      0.82      0.89        49\\n           8       0.85      0.98      0.91        53\\n\\n    accuracy                           0.90       102\\n   macro avg       0.91      0.90      0.90       102\\nweighted avg       0.91      0.90      0.90       102\\n')\n",
      "(0.9019607843137255, 0.9019607843137255, '              precision    recall  f1-score   support\\n\\n           4       0.90      0.90      0.90        49\\n           8       0.91      0.91      0.91        53\\n\\n    accuracy                           0.90       102\\n   macro avg       0.90      0.90      0.90       102\\nweighted avg       0.90      0.90      0.90       102\\n')\n",
      "(0.9607843137254902, 0.9607843137254902, '              precision    recall  f1-score   support\\n\\n           4       0.96      0.96      0.96        49\\n           8       0.96      0.96      0.96        53\\n\\n    accuracy                           0.96       102\\n   macro avg       0.96      0.96      0.96       102\\nweighted avg       0.96      0.96      0.96       102\\n')\n"
     ]
    }
   ],
   "source": [
    "print(knn(X_train_normalized, y_train, X_test_normalized, y_test))\n",
    "print(decision_tree(X_train_normalized, y_train, X_test_normalized, y_test))\n",
    "print(random_forest(X_train_normalized, y_train, X_test_normalized, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2afec9",
   "metadata": {},
   "source": [
    "## Experimentación y evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001dfbae",
   "metadata": {},
   "source": [
    "> Para cada algoritmo, generen al menos 8 experimentos variando los parámetros.  \n",
    "> * Para cada modelo, clasifica el conjunto de entrenamiento y evaluación, recuerda obtener las métricas para cada uno.  \n",
    "> * Muestren la exactitud en gráficas, de tal modo que se puede apreciar si existe overfitting y/o underfitting. Describe las gráficas obtenidas por modelo.  \n",
    "> * Muestra los resultados de la métrica seleccionada por medio de tablas y/o gráficas. Redacta los hallazgos que consideren importantes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caee726d",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32911fe6",
   "metadata": {},
   "source": [
    "> Redacta las conclusiones del experimento, toma en cuenta los siguientes puntos:  \n",
    "> • ¿Qué algoritmo tuvo un mejor desempeño? ¿Con qué parámetros se logró esto?  \n",
    "> • ¿Hay diferencia significativa entre los diferentes algoritmos?  \n",
    "> • ¿Los resultados obtenidos permiten usar estos modelos en sector vitivinícola? ¿Por qué?  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
